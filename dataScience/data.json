{"posts":[{"file":"29_08_2022-examBias.md","date":"29/08/2022","title":"# Exam - Bias","content":"<p>Last week not much happened, so I have not much to talk about as no classes were help (except for the Monday) and I had exams. The only real thing that I did was some study/prep for the exam by going over a few biases and the way they affected data outcomes, that being I did a recap on the studies that I already talked about in previous posts.</p>\n<p>The recap looked over some of the things that I was studying last week and looked over the different effects it had on the outcomes of the data and realisations of the data analysts. I then got into the exam and went back over what I learnt to answer the key question about how misrepresentation of data can be intentional and is the fault of the data analysts or methods/processes involved in collecting the data. I answered this in reference to 3 different misrepresentations of data and how they were affected by the processes involved and how they influenced the interpretation of the data. This was seen to have the possibility of a positive or negative impact, with the intensity varying, whether accidental or on purpose. This was the 'in a nutshell' of which I answered the question and wrote practically an entire essay on. I realised that my time management in this essay was a lot better than others, though, as in my English essay I only got through a very small portion as I took <em>way</em> too much time to plan, where in this one I didn't plan and just started writing as I knew what I was talking about, and I hope to bring this into the future exams and assignments so that I have less stress and am able to get everything done on time.</p>\n<p>This week, I am going to continue working through the classwork provided to grasp a decent understanding of the concepts taught so that I can use my own understanding in future tasks instead of only half learning something so that I won't have to look back over concepts multiple times when I should already have them stuck in my head.</p>","id":"examBias"},{"file":"21_08_2022-bias2.md","date":"21/08/2022","title":"# Bias - The Finale","content":"<p>Biases can be a bit of a hassle in data analysis, as when data that has a bias is analysed and used for something important, it will create a skewed output. The task that was assigned to me was to look into the different data biases and a scenario that has occurred due to the bias. They went as follows:</p>\n<ul>\n<li><p>Response bias</p>\n<ul>\n<li>A study of patient satisfaction surveys in hospitals in 2002</li>\n<li>The aim was to identify and evaluate the presence of any response bias</li>\n<li>This found a correlation between average satisfaction level and response rate<ul>\n<li>The more satisfied someone was, the more likely they were to respond</li></ul></li></ul></li>\n<li><p>Selection bias</p>\n<ul>\n<li>A study was done on the data gathered from a previous study on the estimates of the proportion of the population with autism spectrum disorder (ASD) and an intellectual disability (ID)</li>\n<li>Previous studies showed those who have both ASD and ID was about 50%</li>\n<li>The study used a random effect meta analysis to take into account the slight inconsistencies across how the studies were taken out to determine how many participants didn't have ID</li>\n<li>The selection bias was estimated to have &gt;75% of the participants not having ID &amp; 94% of all participants identified as being on the autism spectrum not having ID</li></ul></li>\n<li><p>Presentation bias</p>\n<ul>\n<li>Search engines show presentation quite prominently, e.g. Google</li>\n<li>2008 Study took user preferences for search results and how important/reliable they were based on their position on the page</li>\n<li>The closer it was to the top, the more reliable it seemed, even though it is sorted by popularity and had little to no reference to reliability</li></ul></li>\n<li><p>Ommitted Variable bias (This one took the longest to find)</p>\n<ul>\n<li>A study was done in a biomechanics lab that assessed the effects of physical activity on bone density</li>\n<li>A quick regressions analysis between the two showed no correlation between the two, when there was an expected positive correlation if all was correct</li>\n<li>Once another key variable was added, weight, the correlation became clearer, as the weight variable was an important variable that was omitted at first</li>\n<li>Once they were both added to the regression model, the results showed the two being statistically significant with a positive correlation with bone density</li></ul></li>\n<li><p>Social Bias</p>\n<ul>\n<li>In social bias, there wasn't necessarily a specific case that was gone over by anyone that I found, but instead I dove into a bit of research on the leadup to the BLM movement</li>\n<li>There wasn't much to look at in terms of the inner thoughts of the people, but I was able to look at the data in the percentage of 911 calls in the U.S. that involved using force with a gun, and the percentage of black residents in a neighbourhood</li>\n<li>There was shown to be a slight increase in the number of calls that involved guns with black officers as the percentage of black residents increased, but it grew at a much faster rate when looking at white officers in these areas.</li>\n<li>The bias links back to stereotypes held by a country and racism that is shown in society that has, although gotten better, still seeped through into today's society </li></ul>\n<p><img src=\"/pictures/2208Figure_1.png\" alt=\"Answering the call\" /></p></li>\n</ul>\n<p>The biases all looked at were interesting and able to give me a more indepth understanding on the issue of bias in data and how it can affect the different ways we analyse and use data which can give us a skewed output. I was able to use my time this week to find the different ways each of the different types of data were used, and also dig a little bit deeper into them, although not explicitly shown in this post. Next week, I will continue to dig into these while preparing for an exam that covers biases, and I will also prepare a few other exams that will also be coming up.</p>","id":"bias2"},{"file":"15_08_2022-bias1.md","date":"15/08/2022","title":"# Continuing to look at bias","content":"<p>Now I say continuing to look at bias, but I hadn't gotten much done last week on looking into biases. As stated last week, I was able to find a study on selection bias that would prove useful for the task I had been given, which was to find scenarios where specific biases had occured and to explain what happened, the effects and discuss if it was done or purpose or not.</p>\n<p>The week went by fast. Not like other weeks, but I feel like I wasn't able to get much done. I spent way too much time on web dev and running through the tutorial on JQuery, which I haven't completed yet, and I left myself with barely any time for looking through the different biases. This is definitely an issue as when I don't get one thing done on time as it has a sort-of domino effect, where it will also affect other tasks. My plan is to combat this by putting a lot more time into the subjects outside of class, and not just for I.T. but also for most of my other classes. This means that this week and in future weeks to come I'll have enough time in and outside of class to get work done. To help myself with this I have changed availabilities at work so that I can have more time to deal with school and get everything done to an acceptable standard.</p>\n<p>I'm lucky that the task was extended another week as I wasn't the only one falling behind, and so the plan is not just to combat time management issues, but also to get the biases task completed and to start on the next topics.</p>","id":"bias1"},{"file":"08_08_2022-bias.md","date":"08/08/2022","title":"# The 5 Types of Bias","content":"<p>There are 5 types of bias in data that were covered in class last week. These 5 are <strong>response bias</strong>, <strong>selection bias</strong>, <strong>presentation bias</strong>, <strong>ommitted variable bias</strong> and <strong>societal bias</strong> and they take into account these things:</p>\n<ul>\n<li><p>Response bias</p>\n<ul>\n<li>Using asked data, or data that requires a response.</li>\n<li>The data is subjective</li>\n<li>The data mainly comes from a few demographics and doesn't show all points of view.</li></ul></li>\n<li><p>Selection bias</p>\n<ul>\n<li>Data is directly for a person/selection</li>\n<li>E.g. advertisements or social media algorithms<ul>\n<li>Shows things that are relevant</li>\n<li>Derives new data from how you respond to it</li></ul></li></ul></li>\n<li><p>Presentation bias</p>\n<ul>\n<li>Representation of data could affect the way that it is collected</li>\n<li>Different fonts, highlighting, bold, size etc. all affect how important something seems</li></ul></li>\n<li><p>Ommitted Variable bias</p>\n<ul>\n<li>Variables that are left out of the data due to privacy concerns or other reasons may affect data</li></ul></li>\n<li><p>Social Bias</p>\n<ul>\n<li>Human inputted data will adopt human bias</li>\n<li>Examples of Social bias are evident in the Twitter TAY bot and similar.</li></ul></li>\n</ul>\n<p>We looked at these biases and were able to get a decent understanding, but to really understand it we had to dive a bit deeper and go into some inquiry of situations where the bias occured. This was what I continued looking into for the week, but didn't have enough time to get into all of the biases.</p>\n<p>The week went by well and I struggled to get through the entire workload I needed to. I didn't finish all of the biases that I was looking for, and only got one of the biases inquired into. The research will continue to get done in this upcoming week and I should be able to get it all done by the end of the week. My time management was something I have talked about previously, and have said I need to work on, and I have gotten better with it, but it is still probably not at the point I need it to be at and so I will continue to work at it until it's at a reasonable point.</p>","id":"bias"},{"file":"01_08_2022-SB0.md","date":"01/08/2022","title":"# Seaborn","content":"<p>Over the last week I was able to delve into Seaborn, the graphing library that is built off of matplotlib that allows for creating different graphs.</p>\n<p>Many different techniques were used in the tutorial of Seaborn as it was taking me through the different graphs available through seaborn and how to use them. This was also paired with cleaning of data and other techniques to make the data more accessible and graphing easier. For example, while displaying the poverty rates per state in the U.S.A. we were able to use the code:</p>\n<pre><code class=\"py language-py\"># Bar Plot\n# Poverty rate of each state\n# Fix any missing data with a value of 0 and type all as a float\npercentage_people_below_poverty_level.poverty_rate.replace(['-'],0.0,inplace = True)\npercentage_people_below_poverty_level.poverty_rate = percentage_people_below_poverty_level.poverty_rate.astype(float)\n# Get all unique values to not have too much overlap\narea_list = list(percentage_people_below_poverty_level['Geographic Area'].unique())\narea_poverty_ratio = []\n# Calculate poverty ratio\nfor i in area_list:\n    x = percentage_people_below_poverty_level[percentage_people_below_poverty_level['Geographic Area']==i]\n    area_poverty_rate = sum(x.poverty_rate)/len(x)\n    area_poverty_ratio.append(area_poverty_rate)\n# Create a new dataframe with the new cleaned and sorted data\ndata = pd.DataFrame({'area_list': area_list,'area_poverty_ratio':area_poverty_ratio})\nnew_index = (data['area_poverty_ratio'].sort_values(ascending=False)).index.values\nsorted_data = data.reindex(new_index)\n\n# visualization\n\n# Create a figure and barplot\nplt.figure(figsize=(15,10))\nsns.barplot(x=sorted_data['area_list'], y=sorted_data['area_poverty_ratio'])\nplt.xticks(rotation= 45)\n# Label the axis\nplt.xlabel('States')\nplt.ylabel('Poverty Rate')\n# Title\nplt.title('Poverty Rate Given States')\n\n# Display plot\nplt.show()\n</code></pre>\n<p>The code comments explain it all, but in a nutshell the data is cleaned by replacing any missing data with a value of 0.0 and all data is converted to a float. From there a list is created and each area's poverty ratio is stored. The data is then sorted and put into a new dataframe to be re-indexed and fed into the plot which is then displayed in an easy to understand manner. It also created this output:</p>\n<p><img src=\"/pictures/Figure_1_8.png\" alt=\"bar graph\" /></p>\n<p>This has given me insight into the different methods I can use to use seaborn and plain old python to clean, organise and show data in a nice way. It has given me insight into the different methods available and also through the use of conversing with others in the course, I have been able to see different approaches to problems that I would most likely do in the most difficult way possible, <i>oops</i>. Either way, it helps me understand what I need to focus on in future lessons to develop necessary skills for data science if I do or don't want to follow a career route in it.</p>","id":"SB0"},{"file":"24_07_2022-bigData.md","date":"24/07/2022","title":"# Big Data","content":"<p>This week I got an introduction to big data. In a nutshell, big data is data in great quantities (over 1 Terrabytes worth at minimum). There are 4 Vs of big data that give it meaning (Vs is not Versus, there are literally 4 words that start with V). Finally the week was all topped off with looking at some code and not having enough time to write any code myself, but that will also be done next week.</p>\n<p>The value of big data is determined by the 4 big Vs of data.</p>\n<p>They can be defined in the following ways:</p>\n<ul>\n<li><p>Volume </p>\n<ul>\n<li>The amount of data available</li>\n<li>The more data available, the more valuable.</li></ul></li>\n<li><p>Velocity</p>\n<ul>\n<li>How fast we can acquire said data</li>\n<li>Real time data is more valuable</li></ul></li>\n<li><p>Variety</p>\n<ul>\n<li>Different types of data</li>\n<li>More variety in the types of data includes different sources<ul>\n<li>Not just one type e.g. Social media, but also other data like sales data or population data etc.</li></ul></li>\n<li>Variations in form of data<ul>\n<li>Structured<ul>\n<li>Spreadsheets etc.</li></ul></li>\n<li>Unstructured<ul>\n<li>Wikis, documents, forms etc.</li></ul></li></ul></li></ul></li>\n<li><p>Veracity</p>\n<ul>\n<li>Elimination of abnormalities, bias, noise or irrelevant data.</li>\n<li>Cleaning the data allows for precise data to affect results</li></ul></li>\n</ul>\n<p>These 4 Vs can be combined to create a 5th V, that being Value. If the data has a lot of volume, can be quickly and easily acquired, is up to date, comes in different forms and has little to no abnormalities, bias, noise or irrelevant data, then it will have a higher value to people.</p>\n<p>There was also a small overview for an intro back into seaborn, but I was only able to look into it briefly as it was introduced towards the end of the week and so I focused on the CSS Frameworks for the majority of the week up until the intro to Big Data and Seaborn.</p>\n<p>There have been a few outcomes from this week that I'm happy with. I am firstly glad that I was able to dive into these concepts with a positive mindset and with the intent to understand what is being taught and to enjoy learning about data science components/concepts, in specifics for this semester, big data. Secondly I've been able to develop a decent understanding of what is being taught. I have been able to constantly listen to what is being taught and attempt to connect the dots to previous lessons and concepts that have been taught, and for the things I'm not understanding I have been able to ask questions to further inquire into the specific concept. This has helped in boosting my confidence in the course and allowed me to feel a bit more stress-free so that I can further develop my understanding leading up to the first assessment item.</p>","id":"bigData"},{"file":"06_06_2022-dataScienceAssignment2.md","date":"06/06/2022","title":"# Completing the Assignment","content":"<p>Over the last week I got a lot done in my assignment, getting the codebase of the assignment done all in one day because I just got into a groove. This was great as for quite a while I hadn't gotten much done on the assignment and realised this in my blog post last week, saying that I needed to work on getting more done on the assignment and splitting my work better between data science and web dev.</p>\n<p>This week I developed the codebase for the assignment and started on the discussion, writing all the helper functions I needed and being able to get a decent looking graph that would allow me to get analysis done. This took me a decent chunk of time but in the end it was well worth it, being able to look through the set of data and looking at useful ways of graphing it. From there I had to look at cleaning it and getting into the right format to work with Seaborn and Numpy so that it could be graphed effectively. The  helper functions for the code were these:</p>\n<pre><code class=\"py language-py\">def formatDates(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Format the date in the date column\n    to be formatted as YYYY instead of MM-YY\n\n    Parameters:\n        df: pd.DataFrame\n\n    Returns:\n        df: pd.DataFrame\n    \"\"\"\n    dates = []\n    for row in df[\"Date\"]:\n        _, year = row.split(\"-\")\n        # If year is greater than 50, it is assumed to be 1951-1999, otherwise it is assumed to be 2000-2050\n        year = '19' + year if year &gt; '50' else '20' + year\n        dates.append(str(np.datetime64(year)))\n    df[\"Date\"] = dates\n    return df\n\n\ndef calculateAverages(df: pd.DataFrame, excluded: list = list()) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate the average of the costs in the row\n\n    Parameters:\n        row: pd.DataFrame\n        excluded: list - list of columns to exclude from calculation\n\n    Returns:\n        average: pd.DataFrame\n    \"\"\"\n\n    averages = []\n    for i in range(len(df[df.columns[0]])):\n        # For length of values in columns\n        columns = []\n        for column in df.columns[::]:\n            if not (column in excluded):\n                # Get mean of all columns except the Date column\n                columns.append(df[column][i])\n        average = np.round(np.mean(np.nan_to_num(columns)), 2)\n        averages.append(average)\n\n    averages = np.nan_to_num(\n        np.array(averages)) if np.nan in averages else np.array(averages)\n    return pd.DataFrame({\"average\": averages})\n\n\ndef indexToPercent(df: pd.DataFrame, column: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert the index number to a percentage\n\n    Parameters:\n        df: pd.DataFrame\n        column: str - column to convert\n\n    Returns:\n        df: pd.DataFrame\n    \"\"\"\n    # pretty much just take away 100 to get the increase\n    values = df[column].values.copy()\n    for i in range(len(df[column].values)):\n        df.loc[i, column] -= values[i-4] if i &gt; 3 else 100\n\n    return df\n\n\ndef deleteUncommonDates(df1: pd.DataFrame, df2: pd.DataFrame, column: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Delete the row with uncommon dates from the dataframe\n\n    Parameters:\n        df1: pd.DataFrame\n        df2: pd.DataFrame - dataframe to compare to\n        column: str - column to delete\n\n    Returns:\n        df: pd.DataFrame\n    \"\"\"\n\n    # Check if dates in df1 are in df2\n    values = df1[column].values\n    for i in range(len(values)):\n        if values[i] not in df2[column].values:\n            df1.drop(i, axis=0, inplace=True)\n    df1.reset_index(drop=True, inplace=True)\n\n    return df1\n</code></pre>\n<p>and the output looked like this:</p>\n<p><img src=\"/pictures/0606Figure1.png\" alt=\"0606Figure1\" /></p>\n<p>With all of this I was able to get into the analysis part, and you can see that there's a big spike at the end of the graph from 2019 onwards, which I suspect was from the property price cycle, where the rate of price increase will change in a specific cycle. This was something that I was able to have a look into but was not able to fully confirm or deny it. This is something I am continually looking into up until the assignment is handed in.</p>\n<p>Although I haven't fully completed the assignment by this point, it will definitely be in by the end of the day, and hopefully without a 5% penalty.</p>\n<p>My goal going forward is to just get things started early, as whenever I leave things until the last minute I am too stressed and normally don't have a good enough assignment for a good mark. This has been seen time and time again when I have done the exact same thing, and I reckon that I am starting to realise the importance of getting it started early. The time I've had hasn't been used to the best that I could have used it, but I have been happy that I have been able to go to classmates about ideas in different pieces of code or elsewhere. Thus this will be my last goal for the semester and will be the goal I go into next semester with: Get it started early.</p>","id":"dataScienceAssignment2"},{"file":"30_05_2022-dataScienceAssignment1.md","date":"30/05/2022","title":"# New approach to the assignment","content":"<p>Last week I started the Data Science assignment, getting into the groove of finding some data sets, comparing them, creating a hypothesis and everything alongside it. I realise that previously the data sets I had chosen were quite simple and straight forward, not leaving much up to using my brain and figuring out what information could extracted from the data that is useful and interesting that would be used for proving or disproving my hypothesis. This week a few things changed about the approach and that's what I'm going to be what I'm putting into this entry today.</p>\n<p>Let's start things off with the first main change, the data sets. I said previously that I was going to look at annual car sales and the change throughout time, but was later told that it was too simple and straightforward, which is completely understandable, it seems that it would be a pretty straight forward answer. This week though I had looked into two seperate questions that I was hoping to find a dataset for. The first one was looking into people's socio-economic status based off of their first names to see if there was any correlation there. Sadly the issue that I ran into while looking into that question was finding the specific data sets. The data sets I needed had to have first names and their associated income, lifestyle or anything else that would indicate their associated socio-economic status. One idea that popped up was to look into a list of names that went to private schools, as they would normally have a slightly higher socio-economic status, but it seemed that the data isn't usually open to the public as I was unable to find any.</p>\n<p>The question that I finally ended up looking into was the question of 'How has housing costs increased in accordance with the cost of living?'. This I thought would be interesting as it isn't too straight forward and would leave room for interpreting the data in different ways and being able to see some of the trends and outliers that would pop up. I am especially curious of what has happened leading up to, and in 2020, as the price of buying a house seemed to start sky-rocketing, but I'll leave the data to decide if that is what actually happened. This provides a great opportunity in my studies to have fun with the data and add a bit of complexity to get a deeper answer into my question and hypothesis.</p>\n<p>Reflecting on the week, I was a bit slow in the Data Science side of things, unlike the Web Dev side. I may have gotten a lot of work done of the Web Dev side of things, but it led to me focusing on that. I realise that I had put too much time into just the Web Dev assignment and need to fix that. This upcoming week I'm going to make sure that I focus on getting both assignments done, dividing up my time so that I don't focus too much on just one assignment, and am able to get through both assignments before the due date.</p>","id":"dataScienceAssignment1"},{"file":"23_05_2022-dataScienceAssignment0.md","date":"23/05/2022","title":"# Data Science Assignment & Matplotlib Part 4","content":"<p>Throughout the past week I was start thinking about what I was going to do for the assignment I have received for Data Science that involved me to obtain a dataset, create a hypothesis and then analyse the data to either prove or disprove my hypothesis, which includes plotting the data in a useful and easy to understand way. The other part of the week was me looking at doing part 4 of the intro to Matplotlib course, which I hadn't gotten entirely completed, but managed to get the first half of done.</p>\n<p>I'll start with the matplotlib stuff first. Through looking into more of the basics of matplotlib I was able to gain a deeper understanding into the different ways I can use matplotlib to plot data in a useful way, which will be useful for my assignment. I looked into the use of three main methods, those being:</p>\n<ul>\n<li><code>ax.margins()</code> - Change the margins for the plot, giving padding by x amount</li>\n<li><code>ax.axis()</code> - Change how the axis will fit into the plot</li>\n<li><code>ax.legend()</code> - Gives a legend for the different graphed lines/sets of data</li>\n</ul>\n<p>Through the use of the three methods I was able to replicate a specific graph looking like this:</p>\n<p><img src=\"/pictures/23Figure_1.png\" alt=\"2 circles graphed with a legend\" /></p>\n<p>The graph presented above was produced by the code:</p>\n<pre><code class=\"py language-py\">import numpy as np\nimport matplotlib.pyplot as plt\n\nplt.style.use('classic')\n\nt = np.linspace(0, 2 * np.pi, 150)\nx1, y1 = np.cos(t), np.sin(t)\nx2, y2 = 2 * x1, 2 * y1\n\ncolors = ['darkred', 'darkgreen']\n\n# Try to plot the two circles, scale the axes as shown and add a legend\n# Hint: it's easiest to combine `ax.axis(...)` and `ax.margins(...)` to scale the axes\nfig, ax = plt.subplots()\nax.plot(x1, y1, color=colors[0], label='Inner')\nax.plot(x2, y2, color=colors[1], label='Outer')\n\nax.margins(y=0.05, x=0.2)\nax.axis(\"tight\")\n\nax.legend()\n\nplt.show()\n</code></pre>\n<p>Secondly I was able to start on the data representation and analysis assignment, which I hadn't gotten much of it done, but I had looked into a few datasets, one of which being of annual car sales and the change throughout time, which is a topic that slightly interests me, and of which I find more interesting than other data sets available to me.</p>\n<p>As stated, I haven't gotten too much of the assignment done but I have had a look into the dataset and already partly developed a hypothesis, that being: As the Australian population increases, the amount of cars sold in Australia will also increase. The only issue is that I'm not 100% sure that I will stick with this dataset as I'm unable to actually find a csv download that would allow me to analyse the data, that will be my goal though for this week- I will try to find a csv file to use or change datasets to something I can analyse more efficiently.</p>\n<p>I believe that I could have used this week slightly better in preparing for the assignment and/or finishing up the part 4 of intro to matplotlib, but I got a decent amount done and that is one of the positives for this week. I will make sure that in future weeks I will get right into doing the assignment so that it is in on time or even early. This will be the challenge I set for myself for the week, and I will attempt to discipline myself to make sure it gets done, as I don't want a repeat of my English assignment.</p>","id":"dataScienceAssignment0"},{"file":"16_05_2022-notMuch0.md","date":"16/05/2022","title":"# A bit sidetracked...","content":"<p>This week I didn't get much work done due to stressing over my English assignment that I left until the last minute. Due to my poor planning and motivation to get the assignment done when I received I started to put it off and procrastinate, leading to the five weeks turning into two, and then into five days, then two and finally almost pulling an all nighter to get it in on time.</p>\n<p>I have realised that my issue lies withing the procrastination for doing something that I enjoy more instead of an assignment I have, which I will try to improve with on my assignments for Data Science and Web Dev. Instead of putting off the assignments until the last week, I will start doing the assignments earlier, that being when I receive them to make sure that the deadline won't pose to create to much stress and become overwhelming due to the lack of work that I did.</p>\n<p>The time that I used well this week, though, was mainly put into finishing up the tasks for Web Dev, and that, I believe, I did well. Although I didn't get it completely finished I did get through a good chunk of it that allowed me to have most of the concepts that it was teaching set in my mind.</p>\n<p>For the next week my main focus is to stay on top of tasks, as I don't want to go through the stress of not getting things done until the last minute. It might help running ideas by classmates or friends, and that's what I plan to do, and that's all as long as I stay on top of it all, and be hard on myself to get it done.</p>","id":"notMuch0"},{"file":"09_05_2022-introMatplotlib1.md","date":"09/05/2022","title":"# More Matplotlib and Numpy","content":"<p>Matplotlib, or Mathematical Plotting Library, and Numpy are extremely useful in data science, and in particular, graphing things. They can take sets of data, and graph them, and with 25 different graphing methods, and a lot more modifications you can make to them, there is bound to be a way to visualise any data set in a meaningful way.</p>\n<p>I looked into continuing on with last week's work in Matplotlib, continuing to consolidate my knowledge and even learn more about the library. I focused on part 2 of the intro to Matplotlib, where I learnt more about using the bar graph, fill, averaging with a line graph and then using the Y-error to check for outliers in the data. The code finalised to be:</p>\n<pre><code class=\"python language-python\">import numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(1)\n\n# Generate data...\ny_raw = np.random.randn(1000).cumsum() + 15\nx_raw = np.linspace(0, 24, y_raw.size)\n\n# Get averages of every 100 samples...\nx_pos = x_raw.reshape(-1, 100).min(axis=1)\ny_avg = y_raw.reshape(-1, 100).mean(axis=1)\ny_err = y_raw.reshape(-1, 100).ptp(axis=1)\n\nbar_width = x_pos[1] - x_pos[0]\n\n# Make a made up future prediction with a fake confidence\nx_pred = np.linspace(0, 30)\ny_max_pred = y_avg[0] + y_err[0] + 2.3 * x_pred\ny_min_pred = y_avg[0] - y_err[0] + 1.2 * x_pred\n\n# Just so you don't have to guess at the colors...\nbarcolor, linecolor, fillcolor = 'wheat', 'salmon', 'lightblue'\n\n# Now you're on your own!\nfig, ax = plt.subplots()\nax.fill_between(x_pred, y_min_pred, y_max_pred, color=fillcolor)\nax.set(title='Future Projection of Altitude', xlim=(0, 30), ylim=(0, 100), xlabel=\"Minutes since class began\", ylabel=\"Snarkiness (snark units)\")\n\nax.plot(x_raw, y_raw, color=linecolor, label='Raw Data')\nax.bar(x_pos, y_avg, yerr=y_err, color=barcolor, label='Data', edgecolor='gray', width=bar_width, ecolor='gray', capsize=5, align='edge')\n\nplt.show()\n</code></pre>\n<p>which produced the output:\n<img src=\"/pictures/08Figure_1.png\" alt=\"09Figure1\" /></p>\n<p>The second graph dealt with using a pcolor mesh or a heat map of sorts, displaying data with colors, the code consisted of:</p>\n<pre><code class=\"python language-python\">#Starter code\nimport numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(1)\n\nplt.style.use('classic')\n\n# Generate random data with different ranges...\ndata1 = np.random.random((10, 10))\ndata2 = 2 * np.random.random((10, 10))\ndata3 = 3 * np.random.random((10, 10))\n\n# Set up our figure and axes...\nfig, axes = plt.subplots(ncols=3, figsize=plt.figaspect(0.5))\naxes[0].set_aspect(1)\naxes[1].set_aspect(1)\naxes[2].set_aspect(1)\nfig.tight_layout() # Make the subplots fill up the figure a bit more...\ncax = fig.add_axes([0.25, 0.1, 0.55, 0.03]) # Add an axes for the colorbar\n\n# Now you're on your own!\naxes[0].invert_yaxis()\naxes[1].invert_yaxis()\naxes[2].invert_yaxis()\nim1 = axes[0].pcolormesh(data1, cmap='viridis', vmax=3, vmin=0)\nim2 = axes[1].pcolormesh(data2, cmap='viridis', vmax=3, vmin=0)\nim3 = axes[2].pcolormesh(data3, cmap='viridis', vmax=3, vmin=0)\nfig.colorbar(im1, cax=cax, orientation='horizontal')\nfig.colorbar(im2, cax=cax, orientation='horizontal')\nfig.colorbar(im3, cax=cax, orientation='horizontal')\nplt.show()\n</code></pre>\n<p>and produced:\n<img src=\"/pictures/08Figure_2.png\" alt=\"09Figure_2\" /></p>\n<p>Looking back over the week I learnt a lot about how to use the Matplotlib library to display information in a useful way that is easily interpretable. I learnt a lot about the different classes and subclasses in Matplotlib and the subclass of Pyplot. Reflecting back, I got through a lot in the time given to me, although I probably could have gotten more done, as also stated in the previous post, I did get through what I needed to. In the events where I needed help, I did go to others for help, and it benefitted me as it gave me a greater understanding of the topic at hand. I'll continue to attempt to spend my time wisely in class and get through the topics that are being focused on.</p>","id":"introMatplotlib1"},{"file":"01_05_2022-introMatplotlib0.md","date":"01/05/2022","title":"# Intro to Matplotlib and Numpy","content":"<p>Over the past week, the data science course has had a look at graphs and graphing data in a meaningful way, or at least that's the goal. For the week, we looked at the basics of graphing information using Matplotlib, a mathematical plotting library which takes data and graphs it in a window. The main things looked over were basic use of using specific functions and exploring the limits of how to use specific graphs.</p>\n<p>A tutorial was used to first understand Matplotlib as we had to follow it and see what functions created what graphs and how they were actually setup. The most important part, for me at least, of the tutorial was the consolidation at the end, where we had to recreate a graph using only a limited amount of code. The graph depicted three cosine waves offset by 1 between each other and were in three subplots below each other. The code that was given to us looked like this:</p>\n<pre><code class=\"python language-python\">import numpy as np\nimport matplotlib.pyplot as plt\n\n# Try to reproduce the figure shown in images/exercise_1-1.png\n\n# Our data...\nx = np.linspace(0, 10, 100)\ny1, y2, y3 = np.cos(x), np.cos(x + 1), np.cos(x + 2)\nnames = ['Signal 1', 'Signal 2', 'Signal 3']\n\n# Can you figure out what to do next to plot x vs y1, y2, and y3 on one figure?\n</code></pre>\n<p>With the intended result (This is also my final product which replicated the intended result) looking like this:\n<img src=\"/pictures/01Figure_1.png\" alt=\"cosine wave graphs plotted in subplots\" /></p>\n<p>and the resulting code being:</p>\n<pre><code class=\"python language-python\">import numpy as np\nimport matplotlib.pyplot as plt\n\n# Try to reproduce the figure shown in images/exercise_1-1.png\n\n# Our data...\nx = np.linspace(0, 10, 100)\ny1, y2, y3 = np.cos(x), np.cos(x + 1), np.cos(x + 2)\nnames = ['Signal 1', 'Signal 2', 'Signal 3']\n\n# Can you figure out what to do next to plot x vs y1, y2, and y3 on one figure?\n\n# Create figure and axes\nfig, axes = plt.subplots(nrows=3)\n\n# Set titles, limit the x and y for the cosine functions to fit nicely on the figure\naxes[0].set(title='Signal 1', xlim=(0, 10), ylim=(-1, 1))\naxes[1].set(title='Signal 2', xlim=(0, 10), ylim=(-1, 1))\naxes[2].set(title='Signal 3', xlim=(0, 10), ylim=(-1, 1))\n\n# Plot the cosine functions\naxes[0].plot(x, y1)\naxes[1].plot(x, y2)\naxes[2].plot(x, y3)\n\n# Show the figure\nplt.show()\n</code></pre>\n<p>The tutorial was quite helpful in educating me on how matplotlib works and how it can be used, and this was only part 1 of 2, which will allow me to consolidate the knowledge I have on matplotlib further in the next week.</p>\n<p>Looking back over the week I realise that I've learnt a decent amount on how to use matplotlib to visualise data, using the tutorial effectively and being able to recreate the graphs shown in the challenges that would aid in developing my understanding of the library. I used my time efficiently for the most part, getting through the first part of the tutorial, and I will try to get better at using my time efficiently, but all in all, I was able to get what needed to get done, done, and understand what I was working on throughout the process. I will aim to improve my efficiency at getting work done, as I did get into a few conversations here and there which kept me from doing my work, and I will also just keep trying to enjoy the class and keep inquiring into different routes of what I learn.</p>","id":"introMatplotlib0"},{"file":"03_04_2022-tayTwitterBot1.md","date":"03/04/2022","title":"# Microsoft's TAY Twitter Bot (Finishing)","content":"<p>This week was mainly just me doing my assignments, that of both research reports, one on Microsoft's TAY Twitter Bot, and the other on the W3C, which I talked about in the previous post. The case study for data science about ethical management had me looking into TAY's Twitter Bot and the ethical management of data.</p>\n<p>TAY's case was pretty straight forward, Microsoft made a Twitter Bot in 2016 that used machine learning to improve its conversational model through learning from other people's conversations and from talking to others directly. The release went smoothly at first, learning how it was meant to until a few hours in when TAY was introduced to internet trolls who said offensive, inappropriate things to her and used the 'repeat after me' function that allowed for someone to tell TAY to repeat after them, and she'd then tweet what they said. This was the only times she was inappropriate at first, but she quickly learned to say these things at her own free will, quickly adopting an inappropriate, offensive persona. She started to constantly say things that were intended to offend people, ranging from bringing up politics and issues in that category, to going all out Neo-Nazi saying that \"Hitler was right I hate the jews.\" and much more. TAY was taken down in less than 24 hours and Microsoft issued a formal apology for TAY's outbursts, informing that they were going to try to make improvements on TAY's design.</p>\n<p>Reflecting back on the week, I believe that I left my assignments a bit too late, as I still had a lot to do by the weekend, giving me only two days to be able to complete the assignment, and even then I still managed to get a decent chunk of them done, and I believe that they won't be late, but it won't be comfortable in how much time I have to complete it. In the future I'll try to combat this by starting my assignments earlier and put in a lot of effort in staying on track with the schedule for my assignments and getting them done for the deadline.</p>","id":"tayTwitterBot1"},{"file":"27_03_2022-tayTwitterBot.md","date":"27/03/2022","title":"# Microsoft's TAY Twitter Bot","content":"<p>I started looking into my research reports this week that I need to write up for Data Science and Web Dev, but mainly focused on one as I feel it would be too difficult to do both at once. The research report is a case study, and for Data Science it's about the ethical management of data, and the case that I chose to look into was Microsoft's Twitter Bot \"TAY\".</p>\n<p>TAY was a Twitter bot that was meant to connect with the millennial generation, as it started with some knowledge of how to act but learnt through all of its interactions with other people and what other people post or DM TAY.</p>\n<p>All programs that learn through its environment, though, have a risk of going wrong, and when learning from a community like Twitter, it can one of two ways, TAY could become a wholesome bot that relates well with the community, or have internet trolls mess with it and teach the incorrect things which ultimately lead to TAY becoming an extremely racist and biased Twitter bot that leads to they getting taken down and becoming a case study of what not to do. In this case, the second path was selected, and less than 24 hours after TAY's launch, a lot of the community had already seen just how racist, sexist and biased she had become, wherein some instances she was saying that the Holocaust was made up, and in others, saying that she hates all feminists and they should \"all die and burn in hell\", and this is just the tip of the iceberg, next week I will continue to research what became of TAY (other than being shut down) and how this all links back to the ethical management of data.</p>\n<p>Reflecting on this week's work, I believe I did alright and did sufficient research for the start of the task, and although I still need to research more and write up the actual report, I believe I'll be able to have that done by Monday week, which is when it's due. Another thing is that I believe I have done better this week than some others as I had decent motivation to get the work done and with that, I just got in there to get it done, and that's the sort of attitude that I'm hoping to bring into next week with me as I continue this task.</p>","id":"tayTwitterBot"},{"file":"20_03_2022-dataEthics0.md","date":"20/03/2022","title":"# Ethics in Data","content":"<p>Ethics in data is a big thing, as it has no definite meaning but instead plenty of interpretations. This week, that's exactly what I started to dive into, and we started looking into RoboDebt, which was an algorithm used to identify any overpayments and debts of a person in accordance with Centrelink which compared Centrelink payments with averaged income reports.</p>\n<p><strong>Note: I DIDN'T DO THAT MUCH ON THIS DUE TO INTERRUPTIONS OF CLASSES THIS WEEK, MAINLY TO DO WITH THE ATHLETICS CARNIVAL TAKING ONE OF THE LESSONS AWAY FROM ME</strong></p>\n<p>In a nutshell, the RoboDebt scheme was an automated way of figuring debts and overpayments and was used as a way to recover what people owed back to the government, but it had many flaws that led to many accounts of debts larger than they should've been or non-existent debts, annoying many people and making their lives a bit more difficult (To put it in perspective, there were over 470,000 false debts, accruing an estimated $721m in debts). The scheme was later scrapped due to all the errors from using the data and calculating the debts, giving greater debts etc.</p>\n<p>In retrospect on this week, I believe I used the time I had this week well and got some decent information and a start on the research into RoboDebt and the unethical use of the data due to this scheme which ultimately led to the demise of the scheme. I believe I could've gotten more done and that although I used the time well, there is always more that I can do to enhance the quality of work I do, which I'll make sure I take note of for the future weeks.</p>","id":"dataEthics0"},{"file":"13_03_2022-fallingBehind0.md","date":"13/03/2022","title":"# Accidentally falling behind","content":"<p>Reflecting back on this week, I didn't do too well in terms of keeping on track and getting the work done. Although by the end of the week all the tasks that needed to be completed got completed, I still didn't use all the time that was given to me in class wisely. This week, besides getting side tracked, I looked at doing the Internet and the W3c task for web dev, but I got distracted a few times, the first being when students of the year below me (year 10s) did practice for a maths test on the board and put \"year 9 exam\" above it, hoping to scare the year 9s a bit, and so logically, I decided to put a bunch of Specialist Mathematics work up on the board (Hardest maths for year 11), and wrote \"year 10 exam\" hoping it would scare them instead. Sadly though, the tactic didn't work too well, as by the end of it, I had found out that I got two of the year 10s wanting to do double spec next year… oops…</p>\n<p>Besides that I kept getting distracted casually, I'm not fully sure as to why but it will be something I look into avoiding in the coming weeks. This was a bit of a hassle for me as it kept me from getting the work I needed to get done, done. This normally occured as I got curious as to what the robotics class was doing, or talking to a mate, which normally aren't too bad and I don't focus on it for too long, but this week it got a bit more distracting and kept my mind away from the task.</p>\n<p>Finally, next week and the coming weeks I'll try to improve on this by just keeping on track doing my work as soon as I get it and not getting too distracted with what happens around me. I'll also make sure that I know what I'm doing for my tasks and do what I need to, possibly talking to mates along the way to consolidate knowledge, but not in too much of a distracting way.</p>","id":"fallingBehind0"},{"file":"06_03_2022-LGC3.md","date":"06/03/2022","title":"# Logic Circuits and Object Oriented Programming - Bug fix","content":"<p>This week was focused around both finishing up the Logic Circuits and Object Oriented Programming task, which is what I'll be talking about and reflecting on in this post, and the of looking into \"The Internet and the W3C\", which I'll be talking about in the next post. For about half of the week, I had been finalising the object oriented project and making it look pretty while bug fixing and making sure the program worked correctly. </p>\n<p>I'm quite happy with finishing up the project, fixing all the bugs and having it look nice and work correctly, but I'm a little bit annoyed with myself of how long it took to fix some of the bugs which snuck their way into the code. The bug I'm talking about was of course the recursion error, which was painful as stated in the previous post, but all in all was a pretty easy solve. The error originated from the JK Flip Flops requesting each other's Q or Q' value, which then had to run through all the inputs again, calling the other JK Flip Flop, which as you can see would create an infinite loop. The way that I solved this issue was through creating a new variable to track if the JK Flip Flop had already been requested, and if it had, it would just output the Q value already saved without performing the logic with the inputs.</p>\n<pre><code class=\"python language-python\">...\ndef performGateLogic(self):\n    if not self.requested: # self.requested is true if the method has already been called otherwise it's false and is changed to true\n        self.q = self.qN # qN -&gt; Q Next\n        self.requested = True # self.requested changed to true here to signal that it has been requested\n        if self.getPinJ() == 1:\n            self.qN = 1 if self.q == 0 else 1\n        elif self.getPinK() == 1:\n            # You don't need the output of PinK if you have PinJ as they don't affect each other\n            self.qN = 0 if self.q == 1 else 0\n            # print(\"%s self.q -&gt; %s\" %(self.getLabel(), self.q))\n    return self.q\n...\n</code></pre>\n<p>After solving the bug by adding in the requested variable, it was time to make the program look nice in the terminal. First I had to get rid of the debug loop and all of the print() statements throughout the code. Then, I added in the main loop which inputted with an input(\"Button Pressed? \") and would set the pin for the switch to the input(), and grab the output of the final and gate. This would then print(\"Pulse sent\") if the output was 1 or print(\"No Pulse\"), and then it would set the two JK Flip Flops requested variable back to 0, which was the final part of the code that I needed to do to complete it!</p>\n<pre><code class=\"python language-python\">def main():\n\n    ...\n\n    # Debug loop\n    # while True:\n    #     print(\"\\n\\n *** Output: %s ***\" % AndG3.getOutput())\n    #     JKFFB.requested = False\n    #     JKFFT.requested = False\n    #     print(\"\\n\\n New Clock Cycle \\n\\n\")\n\n    # Create Main Loop\n    while True:\n        button_press = int(input(\"Button Pressed? \"))\n        Sw.pin = button_press\n        if AndG3.getOutput() == 1:\n            print(\"Pulse sent\")\n        else:\n            print(\"No Pulse\")\n        JKFFB.requested = False\n        JKFFT.requested = False\n</code></pre>\n<p>Overall, this project taught me a lot about recursion, debugging and logical thinking, and I believe that these have been extremely useful to my future in programming, giving me preparation for when I encounter similar issues or situations that would require dealing with recursion and other bugs that pop up. Besides giving skills of programming and logical thinking, this week has also given me a greater ability to work with people around me to locate the origin of bugs, or developing ideas for programming or just giving another perspective on how code can be implemented. In summary, I have found great value in the focus of this week and believe it has aided in developing my skills in logical thinking, debugging and general understanding.</p>","id":"LGC3"},{"file":"27_02_2022-LGC2.md","date":"27/02/2022","title":"# Logic Circuits and Object Oriented Programming - Research and understanding","content":"<p>Researching and understanding was quite important this week for me, as we dove straight into using Object Oriented Programming to develop a button that stays active for one clock cycle before deactivating, as said in the previous post. This task at first glance seemed a bit daunting as there was a massive reading and understanding component to it. Luckily though, it was pretty straight-forward and not too boring either. The websites and documents we read through ran through the process of turning an idea into a Finite State Machine diagram then into a Logic Circuit diagram and implementing that into a logic circuit.</p>\n<p>We started off by taking the key idea of a button that only stayed active for one clock cycle before deactivating, and turned it into a finite state maching diagram, using just normal descriptions of how it works, and labelling them plainly and to the point. The states were then converted to binary so that it could be placed onto a truth table alongside the input so the functionality of the machine could be mapped out. The mapping out of the machine was shown in multiple ways, as there is never 1 solution, but some can provide a better functionality than others, and the difference in the two was just the use of either D Flip Flops or JK Flip Flops, with the main difference being the D Flip Flop has one input, while the JK Flip Flop has two.</p>\n<p><img src=\"pictures/FSMDiaLab.png\" alt=\"FSM Diagram Labelled\" /></p>\n<p><img src=\"pictures/FSMDiaBin.png\" alt=\"FSM Diagram Binary\" /></p>\n<p><img src=\"pictures/TruthTableD.png\" alt=\"Truth Tables D Flip Flop\" /></p>\n<p><img src=\"pictures/TruthTableJK.png\" alt=\"Truth Tables JK Flip Flop\" /></p>\n<p>After those two steps, I had to use a Karnaugh map to find the boolean functions that produce the inputs to the flip flops. I feel that although I learnt a lot this week to do with specific Flip Flops and finite state machines, I missed some bits of crucial information, one of which was what a Karnaugh map is and how it can be used, but I will just do some more research into them this upcoming week to gain a deeper understanding into how this can be a useful tool for designing circuits or designing how a program will work, and so I looked at it and went to have a quick check of what it was, but didn't find anything that told me what it was in the quick look. Thus I accepted that it existed and had uses, and moved on through the rest of the document.</p>\n<p><img src=\"pictures/JK_Karnaugh.png\" alt=\"JK Flip Flop Karnaugh maps\" /></p>\n<p>To sum it up, I believe that this week has been mostly successful for Researching and Understanding the content in Logic Circuits, Finite State Machines and Object Oriented Programming, but there are still things that I need to dig deeper into to further my understanding, especially of Karnaugh maps at this point. This week has taught me a lot about the uses of FSM Diagrams and how they can be converted in Logic Circuit Diagrams and put into a functioning program.</p>","id":"LGC2"},{"file":"18_02_2022-FSM0.md","date":"18/02/2022","title":"# Finite State Machines","content":"<p>Looking over the progress throughout the week, I feel I was working effectively with my peers and myself, giving my 100% while talking with those next to me to get another perspective on the code, allowing for more options in the routes I can take to complete the tasks. Socialising with those around is an extremely useful tool, allowing for a range of different thoughts on a task, and pushing for creativity, which can be beneficial to give a well layed-out, effective program that produces the desired functionality. Another upside of socialising is being able to run your work by a peer, and seeing how understandable your code is, and how you could improve it based on their views.</p>\n<p>The code below is the finite state machine that was developed this week:</p>\n<pre><code class=\"python language-python\">import math\n\nallowed = [\"10\", \"20\", \"50\", \"1\", \"2\",\"r\"]\nmoney = 0\nwhile True:\n\n    # State: Insert money\n    print(\"\\nMoney in machine: $%s\" % round(money, 2))\n    print(\"\\nState: Collecting Money\\n\")\n    moneyIn = input(\"Please insert money (or 'r' for refund): \")\n    if moneyIn in allowed:\n        if moneyIn.isdigit(): \n            moneyIn = int(moneyIn)\n            if moneyIn &gt;2: moneyIn/=100\n            money += moneyIn\n            money = round(money, 2)\n        else:\n            if math.floor(money/1) &gt; 0:\n                print(\"%s x $1\" % math.floor(money/1))\n                money -= math.floor(money/1)*1\n            if math.floor(money/2) &gt; 0:\n                print(\"%s x $2\" % math.floor(money/2))\n                money -= math.floor(money/2)*2\n            if math.floor(money/0.5) &gt; 0:\n                print(\"%s x 50c\" % math.floor(money/0.5))\n                money -= math.floor(money/0.5)*0.5\n            if math.floor(money/0.2) &gt; 0:\n                print(\"%s x 20c\" % math.floor(money/0.2))\n                money -= math.floor(money/0.2)*0.2\n            if math.floor(money/0.1) &gt; 0:\n                print(\"%s x 10c\" % math.floor(money/0.1))\n                money -= math.floor(money/0.1)*0.1\n            money = 0\n\n    # State: Dispensing\n    if money &gt;= 1:\n        print(\"\\nState: Dispensing\\n\")\n        print(\"Please choose a drink: \")\n        # Make the drinks easier to add, instead of hardcoding, just put it in the list\n        drinks = [\"Coke\",\"Water\",\"Fanta\",\"Sprite\",\"Dr Pepper\"]\n        for i in drinks:\n            print(\"%s. %s\" % (drinks.index(i)+1, i))\n        drink = input(\"&gt; \")\n        # Convert drink number to an int and give the drink\n        drink = int(drink) if drink.isnumeric() else 5\n        if drink &lt;= len(drinks):\n            money -= 1\n            print(\"\\nHere is your %s\" % drinks[drink-1])\n        elif drink &gt; len(drinks):\n            print(\"\\nInvalid drink\")\n            continue\n        # State: Change\n        print(\"\\nState: Giving change\\n\")\n        print(\"Change: \")\n\n        # Check to give the least amount of coins it can\n        if math.floor(money/2) &gt; 0:\n            print(\"%s x $2\" % math.floor(money/2))\n            money -= math.floor(money/2)*2\n        if math.floor(money) &gt; 0:\n            print(\"%s x $1\" % math.floor(money))\n            money -= math.floor(money)\n        if math.floor(money/0.5) &gt; 0:\n            print(\"%s x 50c\" % math.floor(money/0.5))\n            money -= math.floor(money/0.5)*0.5\n        if math.floor(money/0.2) &gt; 0:\n            print(\"%s x 20c\" % math.floor(money/0.2))\n            money -= math.floor(money/0.2)*0.2\n        if math.floor(money/0.1) &gt; 0:\n            print(\"%s x 10c\" % math.floor(money/0.1))\n            money -= math.floor(money/0.1)*0.1\n        money = 0\n</code></pre>","id":"FSM0"}]}